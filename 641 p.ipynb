{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\envs\\msci641\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in d:\\anaconda\\envs\\msci641\\lib\\site-packages (4.28.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in d:\\anaconda\\envs\\msci641\\lib\\site-packages\\huggingface_hub-0.16.4-py3.8.egg (from transformers) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (0.13.0.dev0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.11.0->transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/e3/bd/4c0a4619494188a9db5d77e2100ab7d544a42e76b2447869d8e124e981d8/fsspec-2023.6.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.7.1)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Using cached fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: fsspec\n",
      "Successfully installed fsspec-2023.6.0\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\msci641\\lib\\site-packages (1.25.0)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/9a/f2/0ad053856debbe90c83de1b4f05915f85fd2146f20faf9daa3b320d36df3/pandas-2.0.3-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp39-cp39-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.0.3-cp39-cp39-win_amd64.whl (10.8 MB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3 tzdata-2023.3\n",
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/2d/30/3afb8bcb785653254eb646ff2680ec4d637b40b06f4b046aca17b5e086b0/scikit_learn-1.3.0-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.3.0-cp39-cp39-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from scikit-learn) (1.25.0)\n",
      "Collecting scipy>=1.5.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.5.0 from https://files.pythonhosted.org/packages/96/9b/10048be0c335327077af430c5a6637c0b9e7fe9121a8048836f1bb022a81/scipy-1.11.1-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.11.1-cp39-cp39-win_amd64.whl.metadata (59 kB)\n",
      "     ---------------------------------------- 0.0/59.1 kB ? eta -:--:--\n",
      "     ---------------------------------- ----- 51.2/59.1 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 59.1/59.1 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=2.0.0 from https://files.pythonhosted.org/packages/81/12/fd4dea011af9d69e1cad05c75f3f7202cdcbeac9b712eea58ca779a72865/threadpoolctl-3.2.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.3.0-cp39-cp39-win_amd64.whl (9.3 MB)\n",
      "Using cached scipy-1.11.1-cp39-cp39-win_amd64.whl (44.1 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.3.0 scipy-1.11.1 threadpoolctl-3.2.0\n",
      "Collecting jsonlines\n",
      "  Using cached jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from jsonlines) (22.1.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-3.1.0\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: click in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\msci641\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install jsonlines\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, BertForMaskedLM, AdamW\n",
    "\n",
    "#Define custom dataset\n",
    "class SpoilerDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.posts = []\n",
    "        self.labels = []\n",
    "        self.spoilers = []\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                self.posts.append(data['postText'][0] + \" [SEP] \" + \" \".join(data['targetParagraphs']))\n",
    "                if 'tags' in data:\n",
    "                    if data['tags'][0] == 'phrase':\n",
    "                        self.labels.append(0)\n",
    "                    elif data['tags'][0] == 'passage':\n",
    "                        self.labels.append(1)\n",
    "                    else:\n",
    "                        self.labels.append(2)\n",
    "                if 'spoiler' in data:\n",
    "                    self.spoilers.append(data['spoiler'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encodings = self.tokenizer(self.posts[idx], truncation=True, padding='max_length', max_length=self.max_length,\n",
    "                                   return_tensors='pt')\n",
    "        item = {key: torch.squeeze(val) for key, val in encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = self.labels[idx]\n",
    "        if self.spoilers:\n",
    "            item['spoilers'] = self.spoilers[idx]\n",
    "        return item\n",
    "\n",
    "#define training loop\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != 'spoilers'}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "#define loss evaluation\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = {key: val.to(device) for key, val in batch.items() if key != 'spoilers'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "#define accuracy evaluation\n",
    "def evl_acc(model, dataloader, device):\n",
    "    model.eval()  # switch the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.logits, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "#initialize model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataset = SpoilerDataset('train.jsonl', tokenizer, 512)\n",
    "val_dataset = SpoilerDataset('val.jsonl', tokenizer, 512)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "#train model\n",
    "for epoch in range(5):\n",
    "    train_loss = train(model, train_dataloader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_dataloader, device)\n",
    "    train_acc = evl_acc(model,train_dataloader,device)\n",
    "    val_acc = evl_acc(model,val_dataloader,device)\n",
    "    print(f'Epoch: {epoch + 1}, Train loss: {train_loss}, Val loss: {val_loss}, Train Accuracy:{train_acc}, Val Accuracy:{val_acc}')\n",
    "\n",
    "model.save_pretrained('./model')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\msci641\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "D:\\anaconda\\envs\\msci641\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\envs\\msci641\\lib\\site-packages\\transformers\\models\\encoder_decoder\\modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_Loss:  0.2341787366895005, train_Bleu:0.019562304155386633, val_loss:0.2030882819648832, val_Bleu:0.10155130855558121\n",
      "Epoch: 2, train_Loss:  0.20659009742317722, train_Bleu:0.038623308233637146, val_loss:0.20105948029085993, val_Bleu:0.08267351781902328\n",
      "Epoch: 3, train_Loss:  0.20051007234142162, train_Bleu:0.052216120067962786, val_loss:0.20037772485986352, val_Bleu:0.0442685122291905\n",
      "Epoch: 4, train_Loss:  0.1968097859225236, train_Bleu:0.057968304867180666, val_loss:0.20028146174736322, val_Bleu:0.05770751499782942\n",
      "Epoch: 5, train_Loss:  0.19308153297868558, train_Bleu:0.05802414610192202, val_loss:0.20082444941625, val_Bleu:0.05206817719418919\n",
      "Epoch: 6, train_Loss:  0.1893930799991358, train_Bleu:0.05557095756669179, val_loss:0.19999172817915678, val_Bleu:0.037674375729368645\n",
      "Epoch: 7, train_Loss:  0.18505812409799546, train_Bleu:0.052794544516083224, val_loss:0.2032849051617086, val_Bleu:0.0528049819306212\n",
      "Epoch: 8, train_Loss:  0.1802583827485796, train_Bleu:0.047608843439399334, val_loss:0.20529950121417642, val_Bleu:0.04888926153420842\n",
      "Epoch: 9, train_Loss:  0.17504857708350754, train_Bleu:0.04399536610934897, val_loss:0.20893381178379059, val_Bleu:0.04353827988274544\n",
      "Epoch: 10, train_Loss:  0.16928363396960777, train_Bleu:0.04195401880229991, val_loss:0.21382255861535668, val_Bleu:0.0427662015365619\n",
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"# Generate spoiler\\nmodel.eval()\\ninput_data = next(iter(test_loader))\\ninputs = {key: val.to(device) for key, val in input_data.items()}\\ngenerated_ids = model.generate(input_ids=inputs['input_ids'], max_length=150, num_beams=2)\\ngenerated_spoilers = [tokenizer.decode(generated_id, skip_special_tokens=True, clean_up_tokenization_spaces=True) for generated_id in generated_ids]\\n\\nfor spoiler in generated_spoilers:\\n    print(spoiler)\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, EncoderDecoderModel, AdamW\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "#define custom dataset\n",
    "class SpoilerDataset(Dataset):\n",
    "    def __init__(self, file, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.posts = []\n",
    "        self.spoilers = []\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                self.posts.append(data['postText'][0] + \" [SEP] \" + \" \".join(data['targetParagraphs']))\n",
    "                if 'spoiler' in data:\n",
    "                    self.spoilers.append(data['spoiler'][0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.posts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.tokenizer(self.posts[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        if idx < len(self.spoilers):\n",
    "            targets = self.tokenizer(self.spoilers[idx], truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        else:\n",
    "            targets = self.tokenizer(\"\", truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        inputs['labels'] = targets['input_ids']\n",
    "        return inputs\n",
    "    \n",
    "def bleu_score(references, predictions):\n",
    "    smoothing = SmoothingFunction().method4\n",
    "    bleu_score_1gram = corpus_bleu(references, predictions, weights=(1, 0, 0, 0), smoothing_function=smoothing)\n",
    "    bleu_score_2gram = corpus_bleu(references, predictions, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothing)\n",
    "    bleu_score_3gram = corpus_bleu(references, predictions, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothing)\n",
    "    bleu_score_4gram = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothing)\n",
    "    bleu_score = (bleu_score_1gram + bleu_score_2gram + bleu_score_3gram + bleu_score_4gram) / 4\n",
    "    return bleu_score\n",
    "\n",
    "#define training function\n",
    "def train_model(model, dataloader, tokenizer, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_references = []\n",
    "    train_predictions= []\n",
    "   \n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        inputs = {key: val.reshape(val.shape[0], -1).to(device) for key, val in batch.items()}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        references = [tokenizer.decode(ref, skip_special_tokens=True, clean_up_tokenization_spaces = True) for ref in inputs ['input_ids']]\n",
    "        predictions = [tokenizer.decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=True) for pred in outputs.logits.argmax(dim=-1)]\n",
    "        train_references.extend([ref.split() for ref in references])\n",
    "        train_predictions.extend([pred.split() for pred in predictions])\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    \n",
    "        \n",
    "    train_bleu_score = bleu_score(train_references, train_predictions)\n",
    "    train_avg_loss = total_loss/len(dataloader)\n",
    "    return train_avg_loss, train_bleu_score\n",
    "\n",
    "#define vlaidation funciton\n",
    "def evaluate_model(model, dataloader, tokenizer, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    val_references = []\n",
    "    val_predictions = []\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            inputs = {key: val.reshape(val.shape[0], -1).to(device) for key, val in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            references = [tokenizer.decode(ref, skip_special_tokens=True, clean_up_tokenization_spaces = True) for ref in inputs ['input_ids']]\n",
    "            predictions = [tokenizer.decode(pred, skip_special_tokens=True, clean_up_tokenization_spaces=True) for pred in outputs.logits.argmax(dim=-1)]\n",
    "            val_references.extend([ref.split() for ref in references])\n",
    "            val_predictions.extend([pred.split() for pred in predictions])\n",
    "            total_loss += loss.item()\n",
    "\n",
    "     \n",
    "    val_bleu_score = bleu_score(val_references, val_predictions)\n",
    "    val_avg_loss = total_loss/len(dataloader)\n",
    "    return val_avg_loss, val_bleu_score\n",
    "\n",
    "\n",
    "\n",
    "# setup parameters\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_length = 512\n",
    "batch_size = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load data\n",
    "train_data = SpoilerDataset('train.jsonl', tokenizer, max_length)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data = SpoilerDataset('val.jsonl',tokenizer,max_length)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size,shuffle=True)\n",
    "test_data = SpoilerDataset('test.jsonl', tokenizer, max_length)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Initialize model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-uncased', 'gpt2')\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model = model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    " \n",
    "# Training loops\n",
    "\n",
    "for epoch in range(10):\n",
    "    train_avg_loss, train_bleu_score = train_model(model, train_loader, tokenizer, optimizer, device)\n",
    "    val_avg_loss, val_bleu_score = evaluate_model(model, val_loader, tokenizer, device)\n",
    "    print(f'Epoch: {epoch+1}, train_Loss:  {train_avg_loss}, train_Bleu:{train_bleu_score}, val_loss:{val_avg_loss}, val_Bleu:{val_bleu_score}')\n",
    "\n",
    "    torch.save(model.state_dict(), f'model_{epoch}.pt')\n",
    "    \n",
    "                   \n",
    "print(\"Training completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
